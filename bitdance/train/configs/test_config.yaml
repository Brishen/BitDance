model:
  vit_patch_size: 16
  encoder:
    max_bs: 2
    vt_forward_func: group
    params:
      gan_decoder: False
      checkpoint: null
      ddconfig:
          double_z: False
          z_channels: 32
          in_channels: 3
          out_ch: 3
          ch: 32
          ch_mult: [1,1,2,2,4]
          num_res_blocks: 1
  llm:
    type: qwen2
    checkpoint: Qwen/Qwen2-0.5B
  head:
    text_pred: standard
    vision_pred:
      type: diffusion_parallel_x
      model_dim: 128
      num_blocks: 2
      num_adaln: 1
      time_shift: 1.
      time_schedule: logit_normal
      diff_batch_mul: 1
      parallel_num: 4
      P_mean: -0.8
      P_std: 0.8
      use_swiglu: False
    vision_perturb: 0.1
    pe_max_len: 1024

data:
  dataset_config_file: null
  vit_patch_size: 16
  num_vit_prefix_tokens: 5
  scale_list: 16
  use_native_resolution: True
  prefetch_factor: 2
  num_workers: 0
  max_num_tokens_per_sample: 1024
  expected_num_tokens: 2048
  max_num_tokens: 2048
  prefer_buffer_before: 1024
  max_buffer_size: 10
  data_seed: 42
  validation:
    path: null
    reso: 256
  text_cond_dropout_prob: 0.1

training:
  results_dir: test_results
  hdfs_results_dir: null
  local_hdfs_checkpoint_dir: null
  checkpoint_dir: null
  wandb_project: null
  wandb_name: test_run
  wandb_runid: 0
  wandb_resume: allow
  wandb_offline: true
  global_seed: 42
  manual_load: null
  auto_resume: false
  resume_from: null
  resume_model_only: false
  log_every: 1
  save_every: 5
  validate_every: 5
  total_steps: 10
  grad_accumulation_steps: 1
  batch_size: 1

  # optimizer
  warmup_steps: 2
  lr_scheduler: constant
  lr: 1e-4
  min_lr: 1e-7
  beta1: 0.9
  beta2: 0.95
  eps: 1e-15
  max_grad_norm: 1.0
  loss_weight_text: 0.01
  loss_weight_vision: 1.0

  # FSDP
  num_replicate: 1
  num_shard: 1
  sharding_strategy: NO_SHARD
  backward_prefetch: BACKWARD_PRE
  cpu_offload: false
